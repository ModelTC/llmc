# llmcï¼šå‘ç²¾ç¡®é«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹å‹ç¼©è¿ˆè¿›

<img src="./imgs/llmc.png" alt="llmc" style="zoom:35%;" />

[![è®¸å¯è¯](https://img.shields.io/badge/è®¸å¯è¯-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) 
[![arXiv](https://img.shields.io/badge/LLM--QBench-2405.06001-b31b1b)](https://arxiv.org/abs/2405.06001)
[![GitHub æ˜Ÿæ ‡](https://img.shields.io/github/stars/ModelTC/llmc.svg?style=social&label=Star&maxAge=60)](https://github.com/ModelTC/llmc)
[![Discord Banner](https://img.shields.io/discord/1139835312592392214?logo=discord&logoColor=white)](https://discord.gg/qZKUDfhm)
[![QQ](https://img.shields.io/badge/QQ-EB1923?logo=tencent-qq&logoColor=white)](http://qm.qq.com/cgi-bin/qm/qr?_wv=1027&k=I9IGPWWj8uuRXWH3_ELWjouf6gkIMgUl&authKey=GA3WbFAsm90ePJf%2FCbc7ZyXXq4ShQktlBaLxgqS5yuSPAsr3%2BDKMRdosUiLYoilO&noverify=0&group_code=526192592)

**\[ [English](https://github.com/ModelTC/llmc?tab=readme-ov-file#llmc-towards-accurate-and-efficient-llm-compression) | ä¸­æ–‡ \]**

**llmc** æ˜¯ä¸€ä¸ªå³æ’å³ç”¨çš„å·¥å…·ï¼Œæ—¨åœ¨é€šè¿‡æœ€å…ˆè¿›çš„å‹ç¼©ç®—æ³•è¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å‹ç¼©ï¼Œä»¥æé«˜æ•ˆç‡å¹¶å‡å°æ¨¡å‹å¤§å°ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ€§èƒ½ã€‚

**è‹±æ–‡æ–‡æ¡£**åœ¨[è¿™é‡Œ](https://llmc-en.readthedocs.io/en/latest/).

**ä¸­æ–‡æ–‡æ¡£**åœ¨[è¿™é‡Œ](https://llmc-zhcn.readthedocs.io/en/latest/).

## æ–°é—»

* **2024å¹´7æœˆ14å·ï¼š**ğŸ”¥æˆ‘ä»¬ç°åœ¨å·²ç»æ”¯æŒäº†æ—‹è½¬ç±»é‡åŒ–ç®—æ³•QuaRot!

* **2024å¹´7æœˆ4æ—¥:** ğŸ“± æˆ‘ä»¬æä¾›äº†å…¬å¼€çš„è®¨è®ºæ¸ é“. å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œå¯ä»¥åŠ å…¥æˆ‘ä»¬çš„ç¤¾åŒº:
    
    *  [Discordç¾¤](https://discord.gg/qZKUDfhm)
    *  [QQç¾¤](http://qm.qq.com/cgi-bin/qm/qr?_wv=1027&k=I9IGPWWj8uuRXWH3_ELWjouf6gkIMgUl&authKey=GA3WbFAsm90ePJf%2FCbc7ZyXXq4ShQktlBaLxgqS5yuSPAsr3%2BDKMRdosUiLYoilO&noverify=0&group_code=526192592)    
    
* **2024å¹´5æœˆ13æ—¥:** ğŸºğŸºğŸº æˆ‘ä»¬å‘å¸ƒäº†é‡åŒ–åŸºå‡†è®ºæ–‡ï¼š

  [**LLM-QBenchï¼šå¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒåé‡åŒ–çš„æœ€ä½³å®è·µåŸºå‡†**](https://arxiv.org/abs/2405.06001).
  
  [Ruihao Gong*](https://xhplus.github.io/), [Yang Yong*](https://github.com/helloyongyang), [Shiqiao Gu*](https://github.com/gushiqiao), [Yushi Huang*](https://github.com/Harahan), [Yunchen Zhang](https://scholar.google.com/citations?user=glkWFyUAAAAJ&hl=en), [Xianglong LiuğŸ“§](https://xlliu-beihang.github.io/), [Dacheng Tao](https://scholar.google.com/citations?user=RwlJNLcAAAAJ&hl=en)

  (* è¡¨ç¤ºå…±åŒç¬¬ä¸€ä½œè€…, ğŸ“§ è¡¨ç¤ºé€šè®¯ä½œè€….)
  
  <div align=center>
   <img src="./imgs/best_practice.png" alt="comp" width="800" />
  </div>

  æˆ‘ä»¬æ¨¡å—åŒ–å¹¶å…¬æ­£åœ°åŸºå‡†æµ‹è¯•äº†é‡åŒ–æŠ€æœ¯ï¼Œè€ƒè™‘åˆ°æ ¡å‡†æˆæœ¬ã€æ¨ç†æ•ˆç‡å’Œé‡åŒ–ç²¾åº¦ã€‚åœ¨å¤šç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šè¿›è¡Œçš„è¿‘ 600 é¡¹å®éªŒæä¾›äº†ä¸‰ä¸ªæ´è§ï¼š
  å…³äºæ ¡å‡†æ•°æ®ã€ç®—æ³•æµç¨‹å’Œé‡åŒ–é…ç½®é€‰æ‹©ã€‚åŸºäºè¿™äº›æ´è§ï¼Œè®¾è®¡äº†ä¸€ä¸ªæœ€ä½³çš„å¤§å‹è¯­è¨€æ¨¡å‹ PTQ æµç¨‹ï¼Œå®ç°äº†åœ¨å„ç§åœºæ™¯ä¸‹æœ€ä½³çš„ç²¾ç¡®åº¦å’Œæ•ˆç‡æ€§èƒ½å¹³è¡¡ã€‚

* **2024å¹´3æœˆ7æ—¥:** ğŸš€ æˆ‘ä»¬å‘å¸ƒäº†å¼ºå¤§ä¸”é«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹å‹ç¼©å·¥å…·çš„é‡åŒ–éƒ¨åˆ†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„åŸºå‡†è®ºæ–‡å³å°†å‘å¸ƒğŸ˜Šã€‚

## çªå‡ºç‰¹æ€§

* é‡åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚ Llama2-70Bã€OPT-175Bï¼Œå¹¶åœ¨ä»…ä¸€ä¸ª A100/H100/H800 GPUä¸Šè¯„ä¼°å…¶ PPLğŸ’¥ã€‚
* ä¸ºç”¨æˆ·æä¾›é€‰æ‹©çš„æœ€æ–°çš„[ä¸åŸè®ºæ–‡ä»£ç ä»“åº“ç²¾åº¦å¯¹é½](benchmark/align.md)çš„å‹ç¼©ç®—æ³•ï¼Œå¹¶ä¸”ç”¨æˆ·å¯ä»¥åœ¨ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä¸Šä¾æ¬¡ä½¿ç”¨å¤šä¸ªç®—æ³•ğŸ’¥ã€‚
* ç”±æˆ‘ä»¬å·¥å…·é€šè¿‡ç‰¹å®šå‹ç¼©ç®—æ³•å¯¼å‡ºçš„è½¬æ¢æ¨¡å‹ï¼ˆ``save_trans``æ¨¡å¼åœ¨``quant``éƒ¨åˆ†çš„[é…ç½®](#é…ç½®)ï¼‰å¯ä»¥é€šè¿‡å¤šä¸ªåç«¯è¿›è¡Œç®€å•é‡åŒ–ï¼Œå¾—åˆ°ç»è¿‡ç‰¹å®šå‹ç¼©ç®—æ³•ä¼˜åŒ–çš„æ¨¡å‹ï¼Œç›¸åº”çš„åç«¯å¯ä»¥è¿›è¡Œæ¨æ–­ğŸ’¥ã€‚
* æˆ‘ä»¬çš„å‹ç¼©æ¨¡å‹ï¼ˆ``save_lightllm``æ¨¡å¼åœ¨``quant``éƒ¨åˆ†çš„[é…ç½®](#

é…ç½®)ï¼‰å…·æœ‰è¾ƒä½çš„å†…å­˜å ç”¨ï¼Œå¯ä»¥ç›´æ¥é€šè¿‡[Lightllm](https://github.com/ModelTC/lightllm)è¿›è¡Œæ¨æ–­ğŸ’¥ã€‚

## ä½¿ç”¨æ–¹å¼

1. å…‹éš†æ­¤ä»“åº“å¹¶å®‰è£…åŒ…ï¼š

   ```shell
   # å®‰è£…åŒ…
   cd llmc
   pip install -r requirements.txt
   ```

2. å‡†å¤‡æ¨¡å‹å’Œæ•°æ®ã€‚

   ```shell
   # åœ¨ä»huggingfaceä¸‹è½½LLMåï¼ŒæŒ‰ä»¥ä¸‹æ–¹å¼å‡†å¤‡æ ¡å‡†å’Œè¯„ä¼°æ•°æ®ï¼š
   cd tools
   python download_calib_dataset.py --save_path [æ ¡å‡†æ•°æ®è·¯å¾„]
   python download_eval_dataset.py --save_path [è¯„ä¼°æ•°æ®è·¯å¾„] 
   ```

3. é€‰æ‹©ä¸€ä¸ªç®—æ³•æ¥é‡åŒ–ä½ çš„æ¨¡å‹ï¼š

   ```shell
   # è¿™æ˜¯ä¸€ä¸ªå…³äº Awq çš„ä¾‹å­ï¼š
   cd scripts
   # ä¿®æ”¹ bash æ–‡ä»¶ä¸­çš„ llmc è·¯å¾„ï¼Œ``llmc_path``ã€‚ä½ ä¹Ÿå¯ä»¥é€‰æ‹©``llmc/configs/quantization/Awq/``ä¸­çš„ä¸€ä¸ªé…ç½®æ¥é‡åŒ–ä½ çš„æ¨¡å‹ï¼Œæˆ–è€…é€šè¿‡æ›´æ”¹``--config``å‚æ•°åœ¨ run_awq_llama.sh ä¸­ä½¿ç”¨æˆ‘ä»¬æä¾›çš„é…ç½®ã€‚
   bash run_awq_llama.sh
   ```

## é…ç½®

ä¸ºäº†å¸®åŠ©ç”¨æˆ·è®¾è®¡ä»–ä»¬çš„é…ç½®ï¼Œæˆ‘ä»¬ç°åœ¨è§£é‡Šæˆ‘ä»¬åœ¨``llmc/configs/``ä¸‹æä¾›çš„æ‰€æœ‰é…ç½®ä¸­çš„ä¸€äº›é€šç”¨é…ç½®ï¼š

* ``model``:

  ```yaml
  model:
      # ç”¨``llmc/models/*.py``ä¸­çš„ç±»åæ›¿æ¢ã€‚
      type: Llama
      # ç”¨ä½ çš„æ¨¡å‹è·¯å¾„æ›¿æ¢ã€‚
      path: model path 
      torch_dtype: auto
  ```

* ``calib``: 

  ```yaml
  # æ³¨æ„ï¼šä¸€äº›ç®—æ³•ä¸éœ€è¦``calib``ï¼Œå¦‚ naive... æ‰€ä»¥ï¼Œä½ å¯ä»¥ç§»é™¤è¿™éƒ¨åˆ†ã€‚
  calib:
      # ç”¨ä¹‹å‰ä¸‹è½½çš„æ ¡å‡†æ•°æ®åç§°æ›¿æ¢ï¼Œä¾‹å¦‚ï¼Œpilevalã€c4ã€wikitext2 æˆ– ptbã€‚
      name: pileval
      download: False
      # ç”¨ä¹‹å‰ä¸‹è½½çš„æŸä¸ªæ ¡å‡†æ•°æ®çš„è·¯å¾„æ›¿æ¢ï¼Œä¾‹å¦‚ï¼Œpilevalã€c4ã€wikitext2 æˆ– ptbã€‚
      path: calib data path
      n_samples: 128
      bs: -1
      seq_len: 512
      # ç”¨``llmc/data/dataset/specified_preproc.py``ä¸­çš„å‡½æ•°åç§°æ›¿æ¢ã€‚
      preproc: general  
      seed: *seed
  ```

* ``eval``:

  ```yaml
  # å¦‚æœä½ æƒ³è¯„ä¼°ä½ çš„é¢„è®­ç»ƒ/è½¬æ¢/å‡é‡åŒ–æ¨¡å‹çš„ PPLã€‚
  eval:
      # ä½ å¯ä»¥è¯„ä¼°é¢„è®­ç»ƒã€è½¬æ¢ã€å‡é‡åŒ–æ¨¡å‹ï¼Œå¹¶è®¾ç½®ä½ æƒ³è¦è¯„ä¼°çš„ä½ç½®ã€‚
      eval_pos: [pretrain, transformed, fake_quant]
      # ç”¨ä¹‹å‰ä¸‹è½½çš„è¯„ä¼°æ•°æ®çš„åç§°æ›¿æ¢ï¼Œä¾‹å¦‚ï¼Œc4ã€wikitext2ã€ptb æˆ– [c4, wikitext2]ã€‚
      name: wikitext2
      download: False
      path: eval data path
      # å¯¹äº 70B æ¨¡å‹è¯„ä¼°ï¼Œbs å¯ä»¥è®¾ç½®ä¸º 20ï¼Œå¹¶ä¸”å¯ä»¥å°† inference_per_block è®¾ç½®ä¸º Trueã€‚
      # å¯¹äº 7B / 13B æ¨¡å‹è¯„ä¼°ï¼Œbs å¯ä»¥è®¾ç½®ä¸º 1ï¼Œå¹¶ä¸”å¯ä»¥å°† inference_per_block è®¾ç½®ä¸º Falseã€‚
      bs: 1
      inference_per_block: False
      seq_len: 2048
  ```

* ``save``:

  ```yaml
  save:
      # å¦‚æœ``save_trans``ä¸º Trueï¼Œè¿™æ„å‘³ç€ä½ æƒ³è¦å¯¼å‡ºè½¬æ¢æ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œå‚æ•°ä¿®æ”¹çš„æ¨¡å‹ï¼Œå…¶æ€§èƒ½å’Œç»“æ„ä¸åŸå§‹æ¨¡å‹ç›¸åŒï¼Œç”¨æˆ·å¯ä»¥å¯¹è½¬æ¢æ¨¡å‹è¿›è¡Œç®€å•é‡åŒ–ï¼Œä»¥è·å¾—ä¸ç‰¹å®šç®—æ³•é‡åŒ–æ¨¡å‹ç›¸åŒçš„æ€§èƒ½ã€‚
      save_trans: False
      # å¦‚æœ``save_lightllm``ä¸º Trueï¼Œè¿™æ„å‘³ç€ä½ æƒ³è¦å¯¼å‡ºçœŸå®çš„é‡åŒ–æ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œä½ä½æƒé‡å’Œæƒé‡åŠæ¿€æ´»é‡åŒ–å‚æ•°ã€‚
      save_lightllm: False
      # å¦‚æœ``save_fake``ä¸º Trueï¼Œæ„å‘³ç€ä½ æƒ³è¦å¯¼å‡ºå‡é‡åŒ–æ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œå»é‡åŒ–çš„æƒé‡å’Œæ¿€æ´»é‡åŒ–å‚æ•°ã€‚
      save_fake: False
      save_path: ./save

* ``quant``:

  ```yaml
  quant:
      # ç”¨``llmc/compression/quantization/*.py``ä¸­çš„ç±»åæ›¿æ¢ã€‚
      method: OmniQuant
      # ä»…æƒé‡é‡åŒ–æ²¡æœ‰``act``éƒ¨åˆ†ã€‚
      weight:
          bit: 8
          symmetric: True
          # é‡åŒ–ç²’åº¦ï¼šper_channel, per_tensor, per_headï¼ˆä¸æ¨èï¼‰ã€‚
          granularity: per_channel
          group_size: -1
          # æ ¡å‡†ç®—æ³•ï¼šlearnble, mse, ä»¥åŠ minmaxï¼ˆé»˜è®¤ï¼‰ã€‚
          calib_algo: learnable
          # ä½¿ç”¨ç›´é€šä¼°è®¡ï¼ˆStright-Through Estimationï¼‰ï¼Œè¿™å¯¹äºå¯å­¦ä¹ çš„æ ¡å‡†ç®—æ³•æ˜¯å¿…éœ€çš„ã€‚
          ste: True
      act:
          bit: 8
          symmetric: True
          # é‡åŒ–ç²’åº¦ï¼šper_token, per_tensor
          granularity: per_token
          ste: True
          # é™æ€é‡åŒ–ï¼ˆæ ¡å‡†æœŸé—´çš„é‡åŒ–ï¼‰æˆ–åŠ¨æ€é‡åŒ–ï¼ˆæ¨ç†æœŸé—´çš„é‡åŒ–ï¼‰ã€‚
          static: True
      # è¿™éƒ¨åˆ†æ˜¯ä¸ºç‰¹å®šç®—æ³•è®¾è®¡çš„ï¼Œç”¨æˆ·å¯ä»¥å‚è€ƒæˆ‘ä»¬æä¾›çš„ç®—æ³•æ¥è®¾è®¡ä»–ä»¬è‡ªå·±çš„ç®—æ³•ã€‚
      special:
          let: True 
          lwc_lr: 0.01
          let_lr: 0.005
          use_shift: False
          alpha: 0.5
          deactive_amp: True
          epochs: 20
          wd: 0
      # å¦‚æœ quant_out ä¸º Trueï¼Œä½¿ç”¨å‰ä¸€ä¸ªé‡åŒ–å—çš„è¾“å‡ºä½œä¸ºåç»­å—çš„æ ¡å‡†æ•°æ®ã€‚
      quant_out: True

## æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨

âœ… [BLOOM](https://huggingface.co/bigscience/bloom)

âœ… [LLaMA](https://github.com/facebookresearch/llama)

âœ… [LLaMA V2](https://huggingface.co/meta-llama)

âœ… [StarCoder](https://github.com/bigcode-project/starcoder)

âœ… [OPT](https://huggingface.co/docs/transformers/model_doc/opt)

âœ… [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon)

âœ… [InternLM2](https://huggingface.co/internlm)

âœ… [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral)

âœ… [LLaMA V3](https://huggingface.co/meta-llama)

ä½ å¯ä»¥å‚è€ƒ ``llmc/models/*.py`` ä¸‹çš„æ–‡ä»¶æ·»åŠ ä½ è‡ªå·±çš„æ¨¡å‹ç±»å‹ã€‚

## æ”¯æŒçš„ç®—æ³•åˆ—è¡¨

### é‡åŒ–

âœ… Naive

âœ… [AWQ](https://arxiv.org/abs/2306.00978)

âœ… [GPTQ](https://arxiv.org/abs/2210.17323)

âœ… [SmoothQuant](https://arxiv.org/abs/2211.10438)

âœ… [OS+](https://arxiv.org/abs/2304.09145)

âœ… [OmniQuant](https://arxiv.org/abs/2308.13137)

âœ… [NormTweaking](https://arxiv.org/abs/2309.02784)

âœ… [AdaDim](https://arxiv.org/pdf/2309.15531.pdf)

âœ… [QUIK](https://arxiv.org/abs/2310.09259)

âœ… [SpQR](https://arxiv.org/abs/2306.03078)

âœ… [DGQ](https://arxiv.org/abs/2310.04836)

âœ… [OWQ](https://arxiv.org/abs/2306.02272)

âœ… [LLM.int8()](https://arxiv.org/abs/2208.07339)

âœ… [HQQ](https://mobiusml.github.io/hqq_blog/)

âœ… [QuaRot](https://arxiv.org/abs/2404.00456)

### å‰ªæ

è¿™éƒ¨åˆ†å³å°†æ¨å‡ºğŸš€ã€‚ 

## å¾…åŠäº‹é¡¹åˆ—è¡¨

### é‡åŒ–

- [ ] QuIP

- [ ] QuIP#

- [ ] AQLM

**æ³¨æ„:** ä¸€äº›ç‰¹å®šç®—æ³•å¦‚ QUIKã€SpQRï¼Œéœ€è¦ç‰¹æ®Šç¡¬ä»¶æˆ–å†…æ ¸æ”¯æŒï¼Œä¸èƒ½é€šè¿‡å¤šä¸ªåç«¯è¿›è¡Œç®€å•é‡åŒ–ï¼Œç„¶ååˆ©ç”¨è¿™äº›åç«¯è¿›è¡Œæ¨æ–­ã€‚ç„¶è€Œï¼Œç”¨æˆ·ä»ç„¶å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„å·¥å…·è¯„ä¼°è¿™äº›ç®—æ³•åœ¨å…¶ç ”ç©¶ä¸­çš„æ€§èƒ½ã€‚

### å‰ªæ

- [ ] SparseGPT

- [ ] Wanda

- [ ] LLM-Pruner

è¿™éƒ¨åˆ†å³å°†æ¨å‡ºğŸš€ã€‚

### æ–‡æ¡£

- [ ] å‹ç¼©æ¨¡å‹çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ï¼Œç„¶ååˆ©ç”¨å¤šä¸ªåç«¯ï¼Œä¾‹å¦‚ [Lightllm](https://github.com/ModelTC/lightllm), [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)ï¼Œè¿›è¡Œæ¨æ–­ã€‚

- [ ] ä¸åŒç®—æ³•çš„ ``quant`` éƒ¨åˆ†ä¸­çš„``special`` æ–‡æ¡£ã€‚

- [ ] ç”¨æˆ·è‡ªå·±æ·»åŠ æ–°ç®—æ³•çš„æ–‡æ¡£ã€‚

æ›´è¯¦ç»†çš„æ–‡æ¡£å³å°†æ¨å‡ºğŸš€ã€‚

## è‡´è°¢

æˆ‘ä»¬çš„ä»£ç å‚è€ƒäº†ä»¥ä¸‹ä»“åº“ï¼š

* https://github.com/mit-han-lab/llm-awq
* https://github.com/mit-han-lab/smoothquant
* https://github.com/OpenGVLab/OmniQuant
* https://github.com/IST-DASLab/gptq
* https://github.com/ModelTC/Outlier_Suppression_Plus
* https://github.com/IST-DASLab/QUIK
* https://github.com/Vahe1994/SpQR
* https://github.com/ilur98/DGQ
* https://github.com/xvyaward/owq
* https://github.com/TimDettmers/bitsandbytes
* https://github.com/mobiusml/hqq

## æ˜Ÿæ ‡å†å²

[![æ˜Ÿæ ‡å†å²å›¾è¡¨](https://api.star-history.com/svg?repos=ModelTC/llmc&type=Timeline)](https://star-history.com/#ModelTC/llmc&Timeline)

## å¼•ç”¨

å¦‚æœæ‚¨è®¤ä¸ºæˆ‘ä»¬çš„ LLM-QBench è®ºæ–‡/llmc å·¥å…·å¯¹æ‚¨çš„ç ”ç©¶æœ‰ç”¨æˆ–ç›¸å…³ï¼Œè¯·åŠ¡å¿…å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ï¼š

```
@misc{llmc,
   author = {llmc contributors},
   title = {llmc: Towards Accurate and Efficient LLM Compression},
   year = {2024},
   publisher = {GitHub},
   journal = {GitHub repository},
   howpublished = {\url{https://github.com/ModelTC/llmc}},
}

@misc{gong2024llmqbench,
      title={LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models}, 
      author={Ruihao Gong and Yang Yong and Shiqiao Gu and Yushi Huang and Yunchen Zhang and Xianglong Liu and Dacheng Tao},
      year={2024},
      eprint={2405.06001},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```
